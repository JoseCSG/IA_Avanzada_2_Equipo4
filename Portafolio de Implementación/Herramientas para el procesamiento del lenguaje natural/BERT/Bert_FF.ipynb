{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9713279,"sourceType":"datasetVersion","datasetId":5941747},{"sourceId":89513707,"sourceType":"kernelVersion"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Classification Using Transformer Networks (BERT)","metadata":{}},{"cell_type":"markdown","source":"Some initialization:","metadata":{}},{"cell_type":"code","source":"import random\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# enable tqdm in pandas\ntqdm.pandas()\n\n# set to True to use the gpu (if there is one available)\nuse_gpu = True\n\n# select device\ndevice = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\nprint(f'device: {device.type}')\n\n# random seed\nseed = 1222\n\n# set random seed\nif seed is not None:\n    print(f'random seed: {seed}')\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:20.614014Z","iopub.execute_input":"2024-11-20T15:51:20.614260Z","iopub.status.idle":"2024-11-20T15:51:26.595202Z","shell.execute_reply.started":"2024-11-20T15:51:20.614231Z","shell.execute_reply":"2024-11-20T15:51:26.594205Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"device: cuda\nrandom seed: 1222\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Read the train/dev/test datasets and create a HuggingFace `Dataset` object:","metadata":{}},{"cell_type":"code","source":"def read_data(filename):\n    # read csv file\n    df = pd.read_csv(filename, header=None)\n    # add column names\n    df.columns = ['label', 'title', 'description']\n    # make labels zero-based\n    df['label'] -= 1\n    # concatenate title and description, and remove backslashes\n    df['text'] = df['title'] + \" \" + df['description']\n    df['text'] = df['text'].str.replace('\\\\', ' ', regex=False)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:26.596601Z","iopub.execute_input":"2024-11-20T15:51:26.597121Z","iopub.status.idle":"2024-11-20T15:51:26.602094Z","shell.execute_reply.started":"2024-11-20T15:51:26.597078Z","shell.execute_reply":"2024-11-20T15:51:26.601291Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"labels = open('/kaggle/input/namesste/classes.txt').read().splitlines()\ntrain_df = read_data('/kaggle/input/agnews-pytorch-simple-embed-classif-90/AG_NEWS/train.csv')\ntest_df = read_data('/kaggle/input/agnews-pytorch-simple-embed-classif-90/AG_NEWS/test.csv')\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:26.603040Z","iopub.execute_input":"2024-11-20T15:51:26.603322Z","iopub.status.idle":"2024-11-20T15:51:27.621656Z","shell.execute_reply.started":"2024-11-20T15:51:26.603288Z","shell.execute_reply":"2024-11-20T15:51:27.620742Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        label                                              title  \\\n0           2  Wall St. Bears Claw Back Into the Black (Reuters)   \n1           2  Carlyle Looks Toward Commercial Aerospace (Reu...   \n2           2    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n3           2  Iraq Halts Oil Exports from Main Southern Pipe...   \n4           2  Oil prices soar to all-time record, posing new...   \n...       ...                                                ...   \n119995      0  Pakistan's Musharraf Says Won't Quit as Army C...   \n119996      1                  Renteria signing a top-shelf deal   \n119997      1                    Saban not going to Dolphins yet   \n119998      1                                  Today's NFL games   \n119999      1                       Nets get Carter from Raptors   \n\n                                              description  \\\n0       Reuters - Short-sellers, Wall Street's dwindli...   \n1       Reuters - Private investment firm Carlyle Grou...   \n2       Reuters - Soaring crude prices plus worries\\ab...   \n3       Reuters - Authorities have halted oil export\\f...   \n4       AFP - Tearaway world oil prices, toppling reco...   \n...                                                   ...   \n119995   KARACHI (Reuters) - Pakistani President Perve...   \n119996  Red Sox general manager Theo Epstein acknowled...   \n119997  The Miami Dolphins will put their courtship of...   \n119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...   \n119999  INDIANAPOLIS -- All-Star Vince Carter was trad...   \n\n                                                     text  \n0       Wall St. Bears Claw Back Into the Black (Reute...  \n1       Carlyle Looks Toward Commercial Aerospace (Reu...  \n2       Oil and Economy Cloud Stocks' Outlook (Reuters...  \n3       Iraq Halts Oil Exports from Main Southern Pipe...  \n4       Oil prices soar to all-time record, posing new...  \n...                                                   ...  \n119995  Pakistan's Musharraf Says Won't Quit as Army C...  \n119996  Renteria signing a top-shelf deal Red Sox gene...  \n119997  Saban not going to Dolphins yet The Miami Dolp...  \n119998  Today's NFL games PITTSBURGH at NY GIANTS Time...  \n119999  Nets get Carter from Raptors INDIANAPOLIS -- A...  \n\n[120000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>title</th>\n      <th>description</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n      <td>Reuters - Private investment firm Carlyle Grou...</td>\n      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n      <td>Reuters - Authorities have halted oil export\\f...</td>\n      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Oil prices soar to all-time record, posing new...</td>\n      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n      <td>Oil prices soar to all-time record, posing new...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>119995</th>\n      <td>0</td>\n      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n      <td>Pakistan's Musharraf Says Won't Quit as Army C...</td>\n    </tr>\n    <tr>\n      <th>119996</th>\n      <td>1</td>\n      <td>Renteria signing a top-shelf deal</td>\n      <td>Red Sox general manager Theo Epstein acknowled...</td>\n      <td>Renteria signing a top-shelf deal Red Sox gene...</td>\n    </tr>\n    <tr>\n      <th>119997</th>\n      <td>1</td>\n      <td>Saban not going to Dolphins yet</td>\n      <td>The Miami Dolphins will put their courtship of...</td>\n      <td>Saban not going to Dolphins yet The Miami Dolp...</td>\n    </tr>\n    <tr>\n      <th>119998</th>\n      <td>1</td>\n      <td>Today's NFL games</td>\n      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n      <td>Today's NFL games PITTSBURGH at NY GIANTS Time...</td>\n    </tr>\n    <tr>\n      <th>119999</th>\n      <td>1</td>\n      <td>Nets get Carter from Raptors</td>\n      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n      <td>Nets get Carter from Raptors INDIANAPOLIS -- A...</td>\n    </tr>\n  </tbody>\n</table>\n<p>120000 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, eval_df = train_test_split(train_df, train_size=0.9)\ntrain_df.reset_index(inplace=True, drop=True)\neval_df.reset_index(inplace=True, drop=True)\n\nprint(f'train rows: {len(train_df.index):,}')\nprint(f'eval rows: {len(eval_df.index):,}')\nprint(f'test rows: {len(test_df.index):,}')","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:27.624257Z","iopub.execute_input":"2024-11-20T15:51:27.624756Z","iopub.status.idle":"2024-11-20T15:51:28.309798Z","shell.execute_reply.started":"2024-11-20T15:51:27.624710Z","shell.execute_reply":"2024-11-20T15:51:28.308584Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"train rows: 108,000\neval rows: 12,000\ntest rows: 7,600\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\nds = DatasetDict()\nds['train'] = Dataset.from_pandas(train_df)\nds['validation'] = Dataset.from_pandas(eval_df)\nds['test'] = Dataset.from_pandas(test_df)\nds","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:28.311064Z","iopub.execute_input":"2024-11-20T15:51:28.311579Z","iopub.status.idle":"2024-11-20T15:51:30.125243Z","shell.execute_reply.started":"2024-11-20T15:51:28.311549Z","shell.execute_reply":"2024-11-20T15:51:30.124346Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['label', 'title', 'description', 'text'],\n        num_rows: 108000\n    })\n    validation: Dataset({\n        features: ['label', 'title', 'description', 'text'],\n        num_rows: 12000\n    })\n    test: Dataset({\n        features: ['label', 'title', 'description', 'text'],\n        num_rows: 7600\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Tokenize the texts:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntransformer_name = 'bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(transformer_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:30.126291Z","iopub.execute_input":"2024-11-20T15:51:30.126713Z","iopub.status.idle":"2024-11-20T15:51:33.150105Z","shell.execute_reply.started":"2024-11-20T15:51:30.126686Z","shell.execute_reply":"2024-11-20T15:51:33.149291Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb0d149a71d143979e261c054c2e7c72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4920f090468840cd8e67a35fc2f3f6c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db1b6ea96624d54b02157fa1f87562e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd09c2ad8dab4e139dc508a5a1913dce"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize(examples):\n    return tokenizer(examples['text'], truncation=True)\n\ntrain_ds = ds['train'].map(\n    tokenize, batched=True,\n    remove_columns=['title', 'description', 'text'],\n)\neval_ds = ds['validation'].map(\n    tokenize,\n    batched=True,\n    remove_columns=['title', 'description', 'text'],\n)\ntrain_ds.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:33.151131Z","iopub.execute_input":"2024-11-20T15:51:33.153063Z","iopub.status.idle":"2024-11-20T15:51:44.467619Z","shell.execute_reply.started":"2024-11-20T15:51:33.153033Z","shell.execute_reply":"2024-11-20T15:51:44.466704Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/108000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd3da243ca046fc89a8a7c45f797a54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd3442fa02af47fabdd89f9816ed8c43"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        label                                          input_ids  \\\n0           1  [101, 3458, 108, 3614, 132, 7127, 108, 3614, 1...   \n1           0  [101, 158, 119, 156, 119, 20339, 22038, 1116, ...   \n2           0  [101, 10864, 8093, 1116, 15386, 8736, 1130, 23...   \n3           2  [101, 1975, 25401, 1116, 5600, 1106, 4348, 321...   \n4           2  [101, 140, 17030, 16016, 2896, 12853, 3052, 14...   \n...       ...                                                ...   \n107995      1  [101, 12366, 112, 11491, 1110, 3264, 143, 2456...   \n107996      2  [101, 108, 3614, 132, 16409, 22910, 1200, 108,...   \n107997      0  [101, 139, 19268, 1120, 8644, 13530, 1107, 212...   \n107998      0  [101, 8612, 14099, 10498, 1111, 1148, 1159, 11...   \n107999      3  [101, 21906, 14334, 11290, 1116, 2170, 18959, ...   \n\n                                           token_type_ids  \\\n0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n...                                                   ...   \n107995  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n107996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n107997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n107998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n107999  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                           attention_mask  \n0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n...                                                   ...  \n107995  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n107996  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n107997  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n107998  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n107999  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n\n[108000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[101, 3458, 108, 3614, 132, 7127, 108, 3614, 1...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>[101, 158, 119, 156, 119, 20339, 22038, 1116, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>[101, 10864, 8093, 1116, 15386, 8736, 1130, 23...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>[101, 1975, 25401, 1116, 5600, 1106, 4348, 321...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>[101, 140, 17030, 16016, 2896, 12853, 3052, 14...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>107995</th>\n      <td>1</td>\n      <td>[101, 12366, 112, 11491, 1110, 3264, 143, 2456...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>107996</th>\n      <td>2</td>\n      <td>[101, 108, 3614, 132, 16409, 22910, 1200, 108,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>107997</th>\n      <td>0</td>\n      <td>[101, 139, 19268, 1120, 8644, 13530, 1107, 212...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>107998</th>\n      <td>0</td>\n      <td>[101, 8612, 14099, 10498, 1111, 1148, 1159, 11...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>107999</th>\n      <td>3</td>\n      <td>[101, 21906, 14334, 11290, 1116, 2170, 18959, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>108000 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Create the transformer model:","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers.models.bert.modeling_bert import BertModel, BertPreTrainedModel\n\n# https://github.com/huggingface/transformers/blob/65659a29cf5a079842e61a63d57fa24474288998/src/transformers/models/bert/modeling_bert.py#L1486\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n        \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            **kwargs,\n        )\n        cls_outputs = outputs.last_hidden_state[:, 0, :]\n        cls_outputs = self.dropout(cls_outputs)\n        logits = self.classifier(cls_outputs)\n        loss = None\n        if labels is not None:\n            loss_fn = nn.CrossEntropyLoss()\n            loss = loss_fn(logits, labels)\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:44.468590Z","iopub.execute_input":"2024-11-20T15:51:44.468824Z","iopub.status.idle":"2024-11-20T15:51:44.983953Z","shell.execute_reply.started":"2024-11-20T15:51:44.468801Z","shell.execute_reply":"2024-11-20T15:51:44.983204Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(\n    transformer_name,\n    num_labels=len(labels),\n)\n\nmodel = (\n    BertForSequenceClassification\n    .from_pretrained(transformer_name, config=config)\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:44.984862Z","iopub.execute_input":"2024-11-20T15:51:44.985348Z","iopub.status.idle":"2024-11-20T15:51:47.890360Z","shell.execute_reply.started":"2024-11-20T15:51:44.985318Z","shell.execute_reply":"2024-11-20T15:51:47.889689Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41c00a38bb0f461bb0539c2c33b58e62"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create the trainer object and train:","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nnum_epochs = 2\nbatch_size = 24\nweight_decay = 0.01\nmodel_name = f'{transformer_name}-sequence-classification'\n\ntraining_args = TrainingArguments(\n    output_dir=model_name,\n    log_level='error',\n    num_train_epochs=num_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    evaluation_strategy='epoch',\n    weight_decay=weight_decay,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:51:47.892672Z","iopub.execute_input":"2024-11-20T15:51:47.892949Z","iopub.status.idle":"2024-11-20T15:52:07.044598Z","shell.execute_reply.started":"2024-11-20T15:51:47.892921Z","shell.execute_reply":"2024-11-20T15:52:07.043878Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef compute_metrics(eval_pred):\n    y_true = eval_pred.label_ids\n    y_pred = np.argmax(eval_pred.predictions, axis=-1)\n    return {'accuracy': accuracy_score(y_true, y_pred)}","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:52:07.045578Z","iopub.execute_input":"2024-11-20T15:52:07.046101Z","iopub.status.idle":"2024-11-20T15:52:07.050749Z","shell.execute_reply.started":"2024-11-20T15:52:07.046075Z","shell.execute_reply":"2024-11-20T15:52:07.049800Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:52:07.051952Z","iopub.execute_input":"2024-11-20T15:52:07.052345Z","iopub.status.idle":"2024-11-20T15:52:08.996182Z","shell.execute_reply.started":"2024-11-20T15:52:07.052299Z","shell.execute_reply":"2024-11-20T15:52:08.995547Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-20T15:52:08.997075Z","iopub.execute_input":"2024-11-20T15:52:08.997334Z","iopub.status.idle":"2024-11-20T16:43:58.920614Z","shell.execute_reply.started":"2024-11-20T15:52:08.997310Z","shell.execute_reply":"2024-11-20T16:43:58.919672Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 49\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112686977778214, max=1.0â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d224be63393942df89ed7d9484f367fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241120_155229-vgpn8ib1</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/powisa1142-night/huggingface/runs/vgpn8ib1' target=\"_blank\">bert-base-cased-sequence-classification</a></strong> to <a href='https://wandb.ai/powisa1142-night/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/powisa1142-night/huggingface' target=\"_blank\">https://wandb.ai/powisa1142-night/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/powisa1142-night/huggingface/runs/vgpn8ib1' target=\"_blank\">https://wandb.ai/powisa1142-night/huggingface/runs/vgpn8ib1</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4500/4500 51:22, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.179200</td>\n      <td>0.171837</td>\n      <td>0.942500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.104900</td>\n      <td>0.162864</td>\n      <td>0.947583</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4500, training_loss=0.1619327129787869, metrics={'train_runtime': 3108.2942, 'train_samples_per_second': 69.491, 'train_steps_per_second': 1.448, 'total_flos': 1.556839626091776e+16, 'train_loss': 0.1619327129787869, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Evaluate on the test partition:","metadata":{}},{"cell_type":"code","source":"test_ds = ds['test'].map(\n    tokenize,\n    batched=True,\n    remove_columns=['title', 'description', 'text'],\n)\ntest_ds.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2024-11-20T16:43:58.921889Z","iopub.execute_input":"2024-11-20T16:43:58.922257Z","iopub.status.idle":"2024-11-20T16:43:59.661679Z","shell.execute_reply.started":"2024-11-20T16:43:58.922219Z","shell.execute_reply":"2024-11-20T16:43:59.660921Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5720c9588a142918df0b9c58b2c2245"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      label                                          input_ids  \\\n0         2  [101, 11284, 1116, 1111, 157, 151, 12966, 1170...   \n1         3  [101, 1109, 6398, 1110, 1212, 131, 2307, 7219,...   \n2         3  [101, 148, 1183, 119, 1881, 16387, 1116, 4468,...   \n3         3  [101, 11689, 15906, 6115, 12056, 1116, 1370, 2...   \n4         3  [101, 11917, 8914, 119, 19294, 4206, 1106, 215...   \n...     ...                                                ...   \n7595      0  [101, 5596, 1103, 1362, 5284, 5200, 3234, 1384...   \n7596      1  [101, 159, 7874, 1110, 2709, 1114, 13875, 1556...   \n7597      1  [101, 16247, 2972, 9178, 2409, 4271, 140, 1418...   \n7598      2  [101, 126, 1104, 1893, 8167, 10721, 4420, 1107...   \n7599      2  [101, 142, 2064, 4164, 3370, 1154, 13519, 1116...   \n\n                                         token_type_ids  \\\n0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n...                                                 ...   \n7595  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n7596  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n7597  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n7598  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n7599  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n\n                                         attention_mask  \n0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n...                                                 ...  \n7595  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n7596  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n7597  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n7598  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n7599  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n\n[7600 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>input_ids</th>\n      <th>token_type_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>[101, 11284, 1116, 1111, 157, 151, 12966, 1170...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>[101, 1109, 6398, 1110, 1212, 131, 2307, 7219,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[101, 148, 1183, 119, 1881, 16387, 1116, 4468,...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>[101, 11689, 15906, 6115, 12056, 1116, 1370, 2...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>[101, 11917, 8914, 119, 19294, 4206, 1106, 215...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7595</th>\n      <td>0</td>\n      <td>[101, 5596, 1103, 1362, 5284, 5200, 3234, 1384...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>7596</th>\n      <td>1</td>\n      <td>[101, 159, 7874, 1110, 2709, 1114, 13875, 1556...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>7597</th>\n      <td>1</td>\n      <td>[101, 16247, 2972, 9178, 2409, 4271, 140, 1418...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>7598</th>\n      <td>2</td>\n      <td>[101, 126, 1104, 1893, 8167, 10721, 4420, 1107...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>7599</th>\n      <td>2</td>\n      <td>[101, 142, 2064, 4164, 3370, 1154, 13519, 1116...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7600 rows Ã— 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"output = trainer.predict(test_ds)\noutput","metadata":{"execution":{"iopub.status.busy":"2024-11-20T16:43:59.662887Z","iopub.execute_input":"2024-11-20T16:43:59.663243Z","iopub.status.idle":"2024-11-20T16:44:33.150889Z","shell.execute_reply.started":"2024-11-20T16:43:59.663205Z","shell.execute_reply":"2024-11-20T16:44:33.150029Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"PredictionOutput(predictions=array([[ 0.04872653, -4.3446593 ,  4.095016  , -0.21087903],\n       [-0.13182594, -3.623675  , -2.9268608 ,  6.3065596 ],\n       [ 0.01213044, -3.0985224 , -3.1305702 ,  6.0712485 ],\n       ...,\n       [-0.8701611 ,  6.979305  , -2.5117514 , -3.711188  ],\n       [-1.2446883 , -3.9501538 ,  6.1298947 , -1.4984075 ],\n       [-2.444557  , -3.7132833 ,  2.2244227 ,  2.8555458 ]],\n      dtype=float32), label_ids=array([2, 3, 3, ..., 1, 2, 2]), metrics={'test_loss': 0.17411203682422638, 'test_accuracy': 0.9484210526315789, 'test_runtime': 33.4763, 'test_samples_per_second': 227.026, 'test_steps_per_second': 4.75})"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_true = output.label_ids\ny_pred = np.argmax(output.predictions, axis=-1)\ntarget_names = labels\nprint(classification_report(y_true, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2024-11-20T16:44:33.151879Z","iopub.execute_input":"2024-11-20T16:44:33.152124Z","iopub.status.idle":"2024-11-20T16:44:33.173638Z","shell.execute_reply.started":"2024-11-20T16:44:33.152098Z","shell.execute_reply":"2024-11-20T16:44:33.172832Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       World       0.96      0.96      0.96      1900\n      Sports       0.99      0.99      0.99      1900\n    Business       0.93      0.91      0.92      1900\n    Sci/Tech       0.92      0.93      0.93      1900\n\n    accuracy                           0.95      7600\n   macro avg       0.95      0.95      0.95      7600\nweighted avg       0.95      0.95      0.95      7600\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"El cÃ³digo anterior genera un pipeline de clasificaciÃ³n para texto, usando BERT, para hacerlo, lo que hace es primero conseguir y preparar los datos, dividiiendolos por entrenamiento, prueba y validaciÃ³n, para que el modelo pueda aprender, despues, los tokeniza usando BERT y genera una capa en el modelo para la clasificacion del texto, con la cual finalmente puede entrenar y evaluar el modelo utilizando los datos antetriores, esta pipeline sirve para integrar, tokenizar y ordenar las tareas de clasificacion de texto","metadata":{}}]}